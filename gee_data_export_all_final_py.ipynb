{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eM1mzDclHbtK",
        "sIvbSCFEIkyB",
        "LKXQ0w0OJqw-",
        "RO9axFmJ9uMT",
        "oioFpVM5NtGk",
        "BCBBSd3GNmvR"
      ],
      "authorship_tag": "ABX9TyNsBqdIl/lHRBLrk+wK4xO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/am1235le/GY7720_Dissertation_209041686/blob/main/gee_data_export_all_final_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract GEE Data into Google Storage Data Bucket\n",
        "\n",
        "The following code was sourced and adapted from the Google Earth Engine (GEE) data extract and formatting code made available by Huot et al. (2022), which can be found under the following link:\n",
        "\n",
        "https://github.com/google-research/google-research/tree/master/simulation_research/next_day_wildfire_spread\n",
        "\n",
        "Huot F., Hu R. L., Goyal N., Sankar T., Ihme M. and Chen Y. F. 2022. Next Day Wildfire Spread: A Machine Learning Dataset to Predict Wildfire Spreading From Remote-Sensing Data. IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1-13, 2022, Art no. 4412513, doi: 10.1109/TGRS.2022.3192974."
      ],
      "metadata": {
        "id": "KM_bC9qp9TO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load libraries and connect to Google Drive"
      ],
      "metadata": {
        "id": "eM1mzDclHbtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load required libraries\n",
        "import enum\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import ee\n",
        "from ee import Date\n",
        "from typing import List, Text, Tuple, Dict\n",
        "from google.colab import files\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "from absl.testing import flagsaver"
      ],
      "metadata": {
        "id": "h9Z6GqJEG_08"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "HiowIJqc9eIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set file path for working folder\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Dissertation/"
      ],
      "metadata": {
        "id": "XSDCHRfw9dtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define random seed for later in code\n",
        "random.seed(123)"
      ],
      "metadata": {
        "id": "4KKzyJ9AGzXU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Data Sources for Extract"
      ],
      "metadata": {
        "id": "sIvbSCFEIkyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Source Data Types, including; Source, Bands and Time units\n",
        "# adapted from Huot et al. (2022)\n",
        "\n",
        "# define data categories\n",
        "class DataType(enum.Enum):\n",
        "  ELEVATION_SRTM = 1\n",
        "  VEGETATION_VIIRS = 2\n",
        "  DROUGHT_GRIDMET = 3\n",
        "  WEATHER_RTMA = 4\n",
        "  WEATHER_GRIDMET = 5\n",
        "  FIRE_MODIS = 6\n",
        "  POPULATIONDENSITY = 7\n",
        "  LULC = 8\n",
        "  MODIS_BANDS = 9\n",
        "\n",
        "# specify category sources\n",
        "DATA_SOURCES = {\n",
        "    DataType.ELEVATION_SRTM: 'USGS/SRTMGL1_003',\n",
        "    DataType.VEGETATION_VIIRS: 'NOAA/VIIRS/001/VNP13A1',\n",
        "    DataType.DROUGHT_GRIDMET: 'GRIDMET/DROUGHT',\n",
        "    DataType.WEATHER_RTMA: 'NOAA/NWS/RTMA',\n",
        "    DataType.WEATHER_GRIDMET: 'IDAHO_EPSCOR/GRIDMET',\n",
        "    DataType.FIRE_MODIS: 'MODIS/061/MOD14A1',\n",
        "    DataType.POPULATIONDENSITY: 'CIESIN/GPWv411/GPW_Population_Density',\n",
        "    DataType.LULC: 'GOOGLE/DYNAMICWORLD/V1',\n",
        "    DataType.MODIS_BANDS: 'MODIS/061/MOD09GA'\n",
        "}\n",
        "\n",
        "# specify category bands from source\n",
        "DATA_BANDS = {\n",
        "    DataType.ELEVATION_SRTM: ['elevation'],\n",
        "    DataType.VEGETATION_VIIRS: ['NDVI'],\n",
        "    DataType.DROUGHT_GRIDMET: ['pdsi'],\n",
        "    DataType.WEATHER_RTMA: [\n",
        "        'PRES',\n",
        "        'TMP',\n",
        "        'UGRD',\n",
        "        'VGRD',\n",
        "        'SPFH',\n",
        "        'WDIR',\n",
        "        'WIND',\n",
        "        'GUST',\n",
        "    ],\n",
        "    DataType.WEATHER_GRIDMET: [\n",
        "        'pr',\n",
        "        'sph',\n",
        "        'th',\n",
        "        'tmmn',\n",
        "        'tmmx',\n",
        "        'vs',\n",
        "        'erc',\n",
        "    ],\n",
        "    DataType.FIRE_MODIS: ['FireMask'],\n",
        "    DataType.POPULATIONDENSITY: ['populationdensity'],\n",
        "    DataType.LULC: ['label'],\n",
        "    DataType.MODIS_BANDS: [\n",
        "        'sur_refl_b01',\n",
        "        'sur_refl_b02',\n",
        "        'sur_refl_b06',\n",
        "        'sur_refl_b07'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# specify time units in days\n",
        "DATA_TIME_SAMPLING = {\n",
        "    DataType.VEGETATION_VIIRS: 8,\n",
        "    DataType.DROUGHT_GRIDMET: 5,\n",
        "    DataType.WEATHER_RTMA: 2,\n",
        "    DataType.WEATHER_GRIDMET: 2,\n",
        "    DataType.FIRE_MODIS: 1,\n",
        "    DataType.LULC: 1,\n",
        "    DataType.MODIS_BANDS: 1,\n",
        "}"
      ],
      "metadata": {
        "id": "rG247vwlG1pI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check datasources\n",
        "print(DATA_SOURCES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcNOyrhJUOvM",
        "outputId": "c8d43251-8254-4781-dc04-6d05d11ff356"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{<DataType.ELEVATION_SRTM: 1>: 'USGS/SRTMGL1_003', <DataType.VEGETATION_VIIRS: 2>: 'NOAA/VIIRS/001/VNP13A1', <DataType.DROUGHT_GRIDMET: 3>: 'GRIDMET/DROUGHT', <DataType.WEATHER_RTMA: 4>: 'NOAA/NWS/RTMA', <DataType.WEATHER_GRIDMET: 5>: 'IDAHO_EPSCOR/GRIDMET', <DataType.FIRE_MODIS: 6>: 'MODIS/061/MOD14A1', <DataType.POPULATIONDENSITY: 7>: 'CIESIN/GPWv411/GPW_Population_Density', <DataType.LULC: 8>: 'GOOGLE/DYNAMICWORLD/V1', <DataType.MODIS_BANDS: 9>: 'MODIS/061/MOD09GA'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set ectravtion defaults\n",
        "# adapted from Huot et al. (2022)\n",
        "\n",
        "# default resampling scale\n",
        "RESAMPLING_SCALE = {DataType.WEATHER_GRIDMET: 10000}\n",
        "# fire detection band\n",
        "DETECTION_BAND = 'detection'\n",
        "# kernel size\n",
        "DEFAULT_KERNEL_SIZE = 128\n",
        "# sampling resolution\n",
        "DEFAULT_SAMPLING_RESOLUTION = 1000  # Units: meters\n",
        "# train/eval split\n",
        "DEFAULT_EVAL_SPLIT = 0.2\n",
        "# limit per GEE call\n",
        "DEFAULT_LIMIT_PER_EE_CALL = 60\n",
        "# default seed\n",
        "DEFAULT_SEED = 123"
      ],
      "metadata": {
        "id": "oVFuugs7G1bV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set spatial domain for image extraction\n",
        "COORDINATES = {\n",
        "    # Used as input to ee.Geometry.Rectangle()\n",
        "    # Region 1\n",
        "    'US': [[-125, 32], [-102, 42]]\n",
        "    # Region 2\n",
        "    #'US': [[-125, 32], [-114, 42]]\n",
        "}"
      ],
      "metadata": {
        "id": "DLNy51ZqG1O0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define functions for data extraction code"
      ],
      "metadata": {
        "id": "LKXQ0w0OJqw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adapted from Huot et al. (2022)\n",
        "\n",
        "# extract individual GEE image for specified data source and bands\n",
        "def get_image(data_type):\n",
        "  \"\"\"Gets an image corresponding to `data_type`.\n",
        "  Args:\n",
        "    data_type: A specifier for the type of data.\n",
        "  Returns:\n",
        "    The EE image correspoding to the selected `data_type`.\n",
        "  \"\"\"\n",
        "  return ee.Image(DATA_SOURCES[data_type]).select(DATA_BANDS[data_type])\n",
        "\n",
        "# extract GEE image collection for specified data source and bands\n",
        "def get_image_collection(data_type):\n",
        "  \"\"\"Gets an image collection corresponding to `data_type`.\n",
        "  Args:\n",
        "    data_type: A specifier for the type of data.\n",
        "  Returns:\n",
        "    The EE image collection corresponding to `data_type`.\n",
        "  \"\"\"\n",
        "  return ee.ImageCollection(DATA_SOURCES[data_type]).select(\n",
        "      DATA_BANDS[data_type])\n",
        "\n",
        "# extract GEE image collection for specified data source; first available image\n",
        "def get_image_collection_pop(data_type):\n",
        "  \"\"\"Gets an image collection corresponding to `data_type`.\n",
        "  Args:\n",
        "    data_type: A specifier for the type of data.\n",
        "  Returns:\n",
        "    The EE image collection corresponding to `data_type`.\n",
        "  \"\"\"\n",
        "  return ee.ImageCollection(DATA_SOURCES[data_type]).first()"
      ],
      "metadata": {
        "id": "0R-MH3GEG1KZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove image mask\n",
        "# sourced from Huot et al. (2022)\n",
        "def remove_mask(image):\n",
        "  \"\"\"Removes the mask from an EE image.\n",
        "  Args:\n",
        "    image: The input EE image.\n",
        "  Returns:\n",
        "    The EE image without its mask.\n",
        "  \"\"\"\n",
        "  mask = ee.Image(1)\n",
        "  return image.updateMask(mask)"
      ],
      "metadata": {
        "id": "0GfxLkm-G1GQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract feature collection from GEE\n",
        "# sourced from Huot et al. (2022)\n",
        "def export_feature_collection(\n",
        "    feature_collection,\n",
        "    description,\n",
        "    bucket,\n",
        "    folder,\n",
        "    bands,\n",
        "    file_format = 'TFRecord',\n",
        "):\n",
        "  \"\"\"Starts an EE task to export `feature_collection` to TFRecords.\n",
        "  Args:\n",
        "    feature_collection: The EE feature collection to export.\n",
        "    description: The filename prefix to use in the export.\n",
        "    bucket: The name of the Google Cloud bucket.\n",
        "    folder: The folder to export to.\n",
        "    bands: The list of names of the features to export.\n",
        "    file_format: The output file format. 'TFRecord' and 'GeoTIFF' are supported.\n",
        "  Returns:\n",
        "    The EE task associated with the export.\n",
        "  \"\"\"\n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "      collection=feature_collection,\n",
        "      description=description,\n",
        "      bucket=bucket,\n",
        "      fileNamePrefix=os.path.join(folder, description),\n",
        "      fileFormat=file_format,\n",
        "      selectors=bands)\n",
        "  task.start()\n",
        "  return task"
      ],
      "metadata": {
        "id": "aOIP-bzPG1CU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convery GEE images to arrays of defined shape\n",
        "# sourced from Huot et al. (2022)\n",
        "def convert_features_to_arrays(\n",
        "    image_list,\n",
        "    kernel_size = DEFAULT_KERNEL_SIZE,\n",
        "):\n",
        "  \"\"\"Converts a list of EE images into `(kernel_size x kernel_size)` tiles.\n",
        "  Args:\n",
        "    image_list: The list of EE images.\n",
        "    kernel_size: The size of the tiles (kernel_size x kernel_size).\n",
        "  Returns:\n",
        "    An EE image made of (kernel_size x kernel_size) tiles.\n",
        "  \"\"\"\n",
        "  feature_stack = ee.Image.cat(image_list).float()\n",
        "  kernel_list = ee.List.repeat(1, kernel_size)  # pytype: disable=attribute-error\n",
        "  kernel_lists = ee.List.repeat(kernel_list, kernel_size)  # pytype: disable=attribute-error\n",
        "  kernel = ee.Kernel.fixed(kernel_size, kernel_size, kernel_lists)\n",
        "  return feature_stack.neighborhoodToArray(kernel)"
      ],
      "metadata": {
        "id": "W8xTGFeiG0-9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify images containing fire\n",
        "# sourced from Huot et al. (2022)\n",
        "def get_detection_count(\n",
        "    detection_image,\n",
        "    geometry,\n",
        "    sampling_scale = DEFAULT_SAMPLING_RESOLUTION,\n",
        "    detection_band = DETECTION_BAND,\n",
        "):\n",
        "  \"\"\"Counts the total number of positive pixels in the detection image.\n",
        "  Assumes that the pixels in the `detection_band` of `detection_image` are\n",
        "  zeros and ones.\n",
        "  Args:\n",
        "    detection_image: An EE image with a detection band.\n",
        "    geometry: The EE geometry over which to count the pixels.\n",
        "    sampling_scale: The sampling scale used to count pixels.\n",
        "    detection_band: The name of the image band to use.\n",
        "  Returns:\n",
        "    The number of positive pixel counts or -1 if EE throws an error.\n",
        "  \"\"\"\n",
        "  detection_stats = detection_image.reduceRegion(\n",
        "      reducer=ee.Reducer.sum(), geometry=geometry, scale=sampling_scale)\n",
        "  try:\n",
        "    detection_count = int(detection_stats.get(detection_band).getInfo())\n",
        "  except ee.EEException:\n",
        "    # If the number of positive pixels cannot be counted because of a server-\n",
        "    # side error, return -1.\n",
        "    detection_count = -1\n",
        "  return detection_count\n"
      ],
      "metadata": {
        "id": "Lf8DrIfgG07j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract sample images that satisfy specified detection count criteria\n",
        "# sourced from Huot et al. (2022)\n",
        "def extract_samples(\n",
        "    image,\n",
        "    detection_count,\n",
        "    geometry,\n",
        "    sampling_ratio,\n",
        "    detection_band = DETECTION_BAND,\n",
        "    sampling_limit_per_call = DEFAULT_LIMIT_PER_EE_CALL,\n",
        "    resolution = DEFAULT_SAMPLING_RESOLUTION,\n",
        "    seed = DEFAULT_SEED,\n",
        "):\n",
        "  \"\"\"Samples an EE image for positive and negative samples.\n",
        "  Extracts `detection_count` positive examples and (`sampling_ratio` x\n",
        "  `detection_count`) negative examples. Assumes that the pixels in the\n",
        "  `detection_band` of `detection_image` are zeros and ones.\n",
        "  Args:\n",
        "    image: The EE image to extract samples from.\n",
        "    detection_count: The number of positive samples to extract.\n",
        "    geometry: The EE geometry over which to sample.\n",
        "    sampling_ratio: If sampling negatives examples, samples (`sampling_ratio` x\n",
        "      `detection_count`) negative examples. When extracting only positive\n",
        "      examples, set this to zero.\n",
        "    detection_band: The name of the image band to use to determine sampling\n",
        "      locations.\n",
        "    sampling_limit_per_call: The limit on the size of EE calls. Can be used to\n",
        "      avoid memory errors on the EE server side. To disable this limit, set it\n",
        "      to `detection_count`.\n",
        "    resolution: The resolution in meters at which to scale.\n",
        "    seed: The number used to seed the random number generator. Used when\n",
        "      sampling less than the total number of pixels.\n",
        "  Returns:\n",
        "    An EE feature collection with all the extracted samples.\n",
        "  \"\"\"\n",
        "  feature_collection = ee.FeatureCollection([])\n",
        "  num_per_call = sampling_limit_per_call // (sampling_ratio + 1)\n",
        "\n",
        "  # The sequence of sampling calls is deterministic, so calling stratifiedSample\n",
        "  # multiple times never returns samples with the same center pixel.\n",
        "  for _ in range(math.ceil(detection_count / num_per_call)):\n",
        "    samples = image.stratifiedSample(\n",
        "        region=geometry,\n",
        "        numPoints=0,\n",
        "        classBand=detection_band,\n",
        "        scale=resolution,\n",
        "        seed=seed,\n",
        "        classValues=[0, 1],\n",
        "        classPoints=[num_per_call * sampling_ratio, num_per_call],\n",
        "        dropNulls=True)\n",
        "    feature_collection = feature_collection.merge(samples)\n",
        "  return feature_collection\n"
      ],
      "metadata": {
        "id": "Mixi6mUQG04K"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split days in specified period into ranges for train, eval, test split\n",
        "# sourced from Huot et al. (2022)\n",
        "def split_days_into_train_eval_test(\n",
        "    start_date,\n",
        "    end_date,\n",
        "    split_ratio = DEFAULT_EVAL_SPLIT,\n",
        "    window_length_days = 8,\n",
        "):\n",
        "  \"\"\"Splits the days into train / eval / test sets.\n",
        "  Splits the interval between  `start_date` and `end_date` into subintervals of\n",
        "  duration `window_length` days, and divides them into train / eval / test sets.\n",
        "  Args:\n",
        "    start_date: The start date.\n",
        "    end_date: The end date.\n",
        "    split_ratio: The split ratio for the divide between sets, such that the\n",
        "      number of eval time chunks and test time chunks are equal to the total\n",
        "      number of time chunks x `split_ratio`. All the remaining time chunks are\n",
        "      training time chunks.\n",
        "    window_length_days: The length of the time chunks (in days).\n",
        "  Returns:\n",
        "    A dictionary containing the list of start day indices of each time chunk for\n",
        "    each set.\n",
        "  \"\"\"\n",
        "  num_days = int(ee.Date.difference(end_date, start_date, unit='days').getInfo())  # pytype: disable=attribute-error\n",
        "  days = list(range(num_days))\n",
        "  days = days[::window_length_days]\n",
        "  random.shuffle(days)\n",
        "  num_eval = int(len(days) * split_ratio)\n",
        "  split_days = {}\n",
        "  split_days['train'] = days[:-2 * num_eval]\n",
        "  split_days['eval'] = days[-2 * num_eval:-num_eval]\n",
        "  split_days['test'] = days[-num_eval:]\n",
        "  return split_days"
      ],
      "metadata": {
        "id": "PnMHcZY8G00x"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the name of bands that will be extracted\n",
        "# adapted from Huot et al. (2022)\n",
        "def _get_all_feature_bands():\n",
        "  \"\"\"Returns list of all bands corresponding to features.\"\"\"\n",
        "  return (DATA_BANDS[DataType.ELEVATION_SRTM] +\n",
        "          ['populationdensity'] +\n",
        "          DATA_BANDS[DataType.DROUGHT_GRIDMET] +\n",
        "          DATA_BANDS[DataType.VEGETATION_VIIRS] +\n",
        "          DATA_BANDS[DataType.WEATHER_GRIDMET] +\n",
        "          ['PrevFireMask'] +\n",
        "          DATA_BANDS[DataType.WEATHER_RTMA] +\n",
        "          DATA_BANDS[DataType.MODIS_BANDS] +\n",
        "          ['nbr'] +\n",
        "          ['dnbr'] +\n",
        "          ['ndii'] +\n",
        "          ['ndvid'] +\n",
        "          DATA_BANDS[DataType.LULC] +\n",
        "          ['lulc'])"
      ],
      "metadata": {
        "id": "Fe5MM6BGG0xq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify fire band\n",
        "# sourced from Huot et al. (2022)\n",
        "def _get_all_response_bands():\n",
        "  \"\"\"Returns list of all bands corresponding to labels.\"\"\"\n",
        "  return DATA_BANDS[DataType.FIRE_MODIS]"
      ],
      "metadata": {
        "id": "FdLgnC2hG0uR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply index to label to each band\n",
        "# sourced from Huot et al. (2022)\n",
        "def _add_index(i, bands):\n",
        "  \"\"\"Appends the index number `i` at the end of each element of `bands`.\"\"\"\n",
        "  return [f'{band}_{i}' for band in bands]"
      ],
      "metadata": {
        "id": "7FTLHenaG0rJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve image samples for specified train, eval test time windows\n",
        "# adapted from Huot et al. (2022)\n",
        "def _get_all_image_collections():\n",
        "  \"\"\"Gets all the image collections and corresponding time sampling.\"\"\"\n",
        "  image_collections = {\n",
        "      'drought':\n",
        "          get_image_collection(DataType.DROUGHT_GRIDMET),\n",
        "      'vegetation':\n",
        "          get_image_collection(DataType.VEGETATION_VIIRS),\n",
        "      'weather':\n",
        "          get_image_collection(DataType.WEATHER_GRIDMET),\n",
        "      'fire':\n",
        "          get_image_collection(DataType.FIRE_MODIS),\n",
        "      'indexbands':\n",
        "          get_image_collection(DataType.MODIS_BANDS),\n",
        "      'lulc':\n",
        "          get_image_collection(DataType.LULC),\n",
        "      'weatherrtma':\n",
        "          get_image_collection(DataType.WEATHER_RTMA),\n",
        "  }\n",
        "  time_sampling = {\n",
        "      'drought':\n",
        "          DATA_TIME_SAMPLING[DataType.DROUGHT_GRIDMET],\n",
        "      'vegetation':\n",
        "          DATA_TIME_SAMPLING[DataType.VEGETATION_VIIRS],\n",
        "      'weather':\n",
        "          DATA_TIME_SAMPLING[DataType.WEATHER_GRIDMET],\n",
        "      'fire':\n",
        "          DATA_TIME_SAMPLING[DataType.FIRE_MODIS],\n",
        "      'indexbands':\n",
        "          DATA_TIME_SAMPLING[DataType.MODIS_BANDS],\n",
        "      'lulc':\n",
        "          DATA_TIME_SAMPLING[DataType.LULC],\n",
        "      'weatherrtma':\n",
        "          DATA_TIME_SAMPLING[DataType.WEATHER_RTMA],\n",
        "  }\n",
        "  return image_collections, time_sampling"
      ],
      "metadata": {
        "id": "rIGznnpqG0oB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify feature collection extract\n",
        "# sourced from Huot et al. (2022)\n",
        "def _verify_feature_collection(\n",
        "    feature_collection\n",
        "):\n",
        "  \"\"\"Verifies the feature collection is valid.\n",
        "  If the feature collection is invalid, resets the feature collection.\n",
        "  Args:\n",
        "    feature_collection: An EE feature collection.\n",
        "  Returns:\n",
        "    `(feature_collection, size)` a tuple of the verified feature collection and\n",
        "    its size.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    size = int(feature_collection.size().getInfo())\n",
        "  except ee.EEException:\n",
        "    # Reset the feature collection\n",
        "    feature_collection = ee.FeatureCollection([])\n",
        "    size = 0\n",
        "  return feature_collection, size"
      ],
      "metadata": {
        "id": "4bnfjjclG0kY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract and prepare data for defined time window\n",
        "# adapted from Huot et al. (2022)\n",
        "def _get_time_slices(\n",
        "    window_start,\n",
        "    window,\n",
        "    projection,  # Defer calling until called by test code\n",
        "    resampling_scale,\n",
        "    lag = 0,\n",
        "):\n",
        "  \"\"\"Extracts the time slice features.\n",
        "  Args:\n",
        "    window_start: Start of the time window over which to extract data.\n",
        "    window: Length of the window (in days).\n",
        "    projection: projection to reproject all data into.\n",
        "    resampling_scale: length scale to resample data to.\n",
        "    lag: Number of days before the fire to extract the features.\n",
        "  Returns:\n",
        "    A list of the extracted EE images.\n",
        "  \"\"\"\n",
        "  image_collections, time_sampling = _get_all_image_collections()\n",
        "  window_end = window_start.advance(window, 'day')\n",
        "  # extract drought layer\n",
        "  drought = image_collections['drought'].filterDate(\n",
        "      window_start.advance(-lag - time_sampling['drought'], 'day'),\n",
        "      window_start.advance(\n",
        "          -lag, 'day')).median().reproject(projection).resample('bicubic')\n",
        "  # extract vegetation layer\n",
        "  vegetation = image_collections['vegetation'].filterDate(\n",
        "      window_start.advance(-lag - time_sampling['vegetation'], 'day'),\n",
        "      window_start.advance(\n",
        "          -lag, 'day')).median().reproject(projection).resample('bicubic')\n",
        "  # extract weather layers\n",
        "  weather = image_collections['weather'].filterDate(\n",
        "      window_start.advance(-lag - time_sampling['weather'], 'day'),\n",
        "      window_start.advance(-lag, 'day')).median().reproject(\n",
        "          projection.atScale(resampling_scale)).resample('bicubic')\n",
        "  # extract weather rmta layers\n",
        "  weatherrtma = image_collections['weatherrtma'].filterDate(\n",
        "      window_start.advance(-lag - time_sampling['weatherrtma'], 'day'),\n",
        "      window_start.advance(-lag, 'day')).median().reproject(\n",
        "          projection.atScale(resampling_scale)).resample('bicubic')\n",
        "  # extract fire layers\n",
        "  prev_fire = image_collections['fire'].filter(ee.Filter.date(\n",
        "      window_start.advance(-lag - time_sampling['fire'], 'day'),\n",
        "      window_start.advance(-lag, 'day'))).map(remove_mask).max().rename('PrevFireMask')\n",
        "  fire = image_collections['fire'].filter(ee.Filter.date(window_start, window_end)).map(remove_mask).max()\n",
        "  detection = fire.clamp(6, 7).subtract(6).rename('detection')\n",
        "  prev_fire = prev_fire.clamp(6,7).subtract(6)\n",
        "  fire = fire.clamp(6,7).subtract(6)\n",
        "  # extract LULC\n",
        "  lulc = image_collections['lulc'].filter(ee.Filter.date(window_start,window_end)).max()\n",
        "  lulc = lulc.eq(6).rename('lulc')\n",
        "  # extract index bands\n",
        "  prev_indbands = image_collections['indexbands'].filter(ee.Filter.date(\n",
        "      window_start.advance(-lag - time_sampling['indexbands'] - 1, 'day'),\n",
        "      window_start.advance(-lag - time_sampling['indexbands'], 'day'))).max()\n",
        "  indbands = image_collections['indexbands'].filter(ee.Filter.date(\n",
        "      window_start.advance(-lag - time_sampling['indexbands'], 'day'),\n",
        "      window_start)).max()\n",
        "  # calculate indices\n",
        "  # NBR T-2\n",
        "  pnbr = prev_indbands.normalizedDifference(['sur_refl_b02', 'sur_refl_b07']).rename('pnbr')\n",
        "  # NBR T-1\n",
        "  nbr = indbands.normalizedDifference(['sur_refl_b02', 'sur_refl_b07']).rename('nbr')\n",
        "  # DNBR\n",
        "  dnbr = pnbr.subtract(nbr).rename('dnbr')\n",
        "  # NDII\n",
        "  ndii = indbands.normalizedDifference(['sur_refl_b02', 'sur_refl_b06']).rename('ndii')\n",
        "  # NDVI\n",
        "  ndvid = indbands.normalizedDifference(['sur_refl_b02', 'sur_refl_b01']).rename('ndvid')\n",
        "  return [drought, vegetation, weather, weatherrtma, prev_fire, fire, lulc, nbr, dnbr, ndii, ndvid, detection]"
      ],
      "metadata": {
        "id": "G1ZgvbhLG0hQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define consolidated export function to export GEE data"
      ],
      "metadata": {
        "id": "RO9axFmJ9uMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consolidated export function that call sampled GEE images for specified time window and spatial extent\n",
        "# sourced from Huot et al. (2022)\n",
        "def _export_dataset(\n",
        "    bucket,\n",
        "    folder,\n",
        "    prefix,\n",
        "    start_date,\n",
        "    start_days,\n",
        "    geometry,\n",
        "    kernel_size,\n",
        "    sampling_scale,\n",
        "    num_samples_per_file,\n",
        "):\n",
        "  \"\"\"Exports the dataset TFRecord files for wildfire risk assessment.\n",
        "  Args:\n",
        "    bucket: Google Cloud bucket\n",
        "    folder: Folder to which to export the TFRecords.\n",
        "    prefix: Export file name prefix.\n",
        "    start_date: Start date for the EE data to export.\n",
        "    start_days: Start day of each time chunk to export.\n",
        "    geometry: EE geometry from which to export the data.\n",
        "    kernel_size: Size of the exported tiles (square).\n",
        "    sampling_scale: Resolution at which to export the data (in meters).\n",
        "    num_samples_per_file: Approximate number of samples to save per TFRecord\n",
        "      file.\n",
        "  \"\"\"\n",
        "\n",
        "  def _verify_and_export_feature_collection(\n",
        "      num_samples_per_export,\n",
        "      feature_collection,\n",
        "      file_count,\n",
        "      features,\n",
        "  ):\n",
        "    \"\"\"Wraps the verification and export of the feature collection.\n",
        "    Verifies the size of the feature collection and triggers the export when\n",
        "    it is larger than `num_samples_per_export`. Resets the feature collection\n",
        "    and increments the file count at each export.\n",
        "    Args:\n",
        "      num_samples_per_export: Approximate number of samples per export.\n",
        "      feature_collection: The EE feature collection to export.\n",
        "      file_count: The TFRecord file count for naming the files.\n",
        "      features: Names of the features to export.\n",
        "    Returns:\n",
        "      `(feature_collection, file_count)` tuple of the current feature collection\n",
        "        and file count.\n",
        "    \"\"\"\n",
        "    feature_collection, size_count = _verify_feature_collection(\n",
        "        feature_collection)\n",
        "    if size_count > num_samples_per_export:\n",
        "      export_feature_collection(\n",
        "          feature_collection,\n",
        "          description=prefix + '_{:03d}'.format(file_count),\n",
        "          bucket=bucket,\n",
        "          folder=folder,\n",
        "          bands=features,\n",
        "      )\n",
        "      file_count += 1\n",
        "      feature_collection = ee.FeatureCollection([])\n",
        "    return feature_collection, file_count\n",
        "\n",
        "  elevation = get_image(DataType.ELEVATION_SRTM)\n",
        "  end_date = start_date.advance(max(start_days), 'days')\n",
        "  populationdensity = get_image_collection_pop(DataType.POPULATIONDENSITY)\n",
        "  populationdensity = populationdensity.rename('populationdensity')\n",
        "  projection = get_image_collection(DataType.WEATHER_GRIDMET)\n",
        "  projection = projection.first().select(DATA_BANDS[DataType.WEATHER_GRIDMET][0]).projection()\n",
        "  resampling_scale = (RESAMPLING_SCALE[DataType.WEATHER_GRIDMET])\n",
        "\n",
        "  all_days = []\n",
        "  for day in start_days:\n",
        "    for i in range(7):\n",
        "      all_days.append(day + i)\n",
        "\n",
        "  window = 1\n",
        "  sampling_limit_per_call = 60\n",
        "  features = _get_all_feature_bands() + _get_all_response_bands()\n",
        "\n",
        "  file_count = 0\n",
        "  feature_collection = ee.FeatureCollection([])\n",
        "  for start_day in all_days:\n",
        "    window_start = start_date.advance(start_day, 'days')\n",
        "    time_slices = _get_time_slices(window_start, window, projection,\n",
        "                                   resampling_scale)\n",
        "    image_list = [elevation, populationdensity] + time_slices[:-1]\n",
        "    detection = time_slices[-1]\n",
        "    arrays = convert_features_to_arrays(image_list, kernel_size)\n",
        "    to_sample = detection.addBands(arrays)\n",
        "\n",
        "    fire_count = get_detection_count(\n",
        "        detection,\n",
        "        geometry=geometry,\n",
        "        sampling_scale=10 * sampling_scale,\n",
        "    )\n",
        "    if fire_count > 0:\n",
        "      samples = extract_samples(\n",
        "          to_sample,\n",
        "          detection_count=fire_count,\n",
        "          geometry=geometry,\n",
        "          # RE-ADD IF ERROR\n",
        "          sampling_ratio=0,  # Only extracting examples with fire.\n",
        "          sampling_limit_per_call=sampling_limit_per_call,\n",
        "          resolution=sampling_scale,\n",
        "      )\n",
        "      feature_collection = feature_collection.merge(samples)\n",
        "\n",
        "      feature_collection, file_count = _verify_and_export_feature_collection(\n",
        "          num_samples_per_file, feature_collection, file_count, features)\n",
        "  # Export the remaining feature collection\n",
        "  _verify_and_export_feature_collection(0, feature_collection, file_count,\n",
        "                                        features)"
      ],
      "metadata": {
        "id": "nFeHez0PG0d5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to specify export parameters\n",
        "# sourced from Huot et al. (2022)\n",
        "def export_ml_datasets(\n",
        "    bucket,\n",
        "    folder,\n",
        "    start_date,\n",
        "    end_date,\n",
        "    prefix = '',\n",
        "    kernel_size = 128,\n",
        "    sampling_scale = 1000,\n",
        "    eval_split_ratio = 0.125,\n",
        "    num_samples_per_file = 1000,\n",
        "):\n",
        "  \"\"\"Exports the ML dataset TFRecord files for wildfire risk assessment.\n",
        "  Export is to Google Cloud Storage.\n",
        "  Args:\n",
        "    bucket: Google Cloud bucket\n",
        "    folder: Folder to which to export the TFRecords.\n",
        "    start_date: Start date for the EE data to export.\n",
        "    end_date: End date for the EE data to export.\n",
        "    prefix: File name prefix to use.\n",
        "    kernel_size: Size of the exported tiles (square).\n",
        "    sampling_scale: Resolution at which to export the data (in meters).\n",
        "    eval_split_ratio: Split ratio for the divide between training and evaluation\n",
        "      datasets.\n",
        "    num_samples_per_file: Approximate number of samples to save per TFRecord\n",
        "      file.\n",
        "  \"\"\"\n",
        "\n",
        "  split_days = split_days_into_train_eval_test(\n",
        "      start_date, end_date, split_ratio=eval_split_ratio, window_length_days=8)\n",
        "\n",
        "  for mode in ['train', 'eval', 'test']:\n",
        "    sub_prefix = f'{mode}_{prefix}'\n",
        "    _export_dataset(\n",
        "        bucket=bucket,\n",
        "        folder=folder,\n",
        "        prefix=sub_prefix,\n",
        "        start_date=start_date,\n",
        "        start_days=split_days[mode],\n",
        "        geometry=ee.Geometry.Rectangle(COORDINATES['US']),\n",
        "        kernel_size=kernel_size,\n",
        "        sampling_scale=sampling_scale,\n",
        "        num_samples_per_file=num_samples_per_file)"
      ],
      "metadata": {
        "id": "n8WFZvmHG0aO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Earth Engine\n",
        "\n",
        "To extract data using this code will require a GEE account"
      ],
      "metadata": {
        "id": "oioFpVM5NtGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# once an account has been created to access data\n",
        "# run this cell, select account under which access was registered and generate token\n",
        "!earthengine authenticate"
      ],
      "metadata": {
        "id": "eurVuT8rQhZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize GEE\n",
        "ee.Initialize()"
      ],
      "metadata": {
        "id": "7ZzohMiQuT9U"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specify time window and extract data"
      ],
      "metadata": {
        "id": "BCBBSd3GNmvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract time window for data extract\n",
        "start_date = \"2018-06-01\"\n",
        "end_date = \"2018-12-31\""
      ],
      "metadata": {
        "id": "_6GFoDIBRSMO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters and extract data to Google storage bucket\n",
        "# bucket = name of Google stoarge bucket\n",
        "# folder = folder name to be created in Google bucket\n",
        "# prefix = tfrecord filename prefix\n",
        "# kernel_size = image kernel size\n",
        "# sampling_scale = data sampling scale in metres\n",
        "# eval_split_ratio = train, eval, test data split\n",
        "# num_samples_per_file = number of samples per tfrecord\n",
        "export_ml_datasets(bucket='bucket_name',\n",
        "                   folder='folder_name',\n",
        "                   start_date=ee.Date(start_date),\n",
        "                   end_date=ee.Date(end_date),\n",
        "                   prefix='file_prefix',\n",
        "                   kernel_size=64,\n",
        "                   sampling_scale=1000,\n",
        "                   eval_split_ratio=0.1,\n",
        "                   num_samples_per_file=1000)"
      ],
      "metadata": {
        "id": "ASzHzC1oSy0l"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}